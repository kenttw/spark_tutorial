{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Recommendation\n",
    "簡單的推薦系統，單純就文章內容的相似度來比對，並依序推薦相似度最高的3篇文章\n",
    "\n",
    "### 資料集\n",
    "旅遊類以及美妝類文章各五篇\n",
    "- pixnet.txt\n",
    "    - http://chahabi77.pixnet.net/blog/post/436715527\n",
    "    - http://chahabi77.pixnet.net/blog/post/403682269\n",
    "    - http://chahabi77.pixnet.net/blog/post/354943724\n",
    "    - http://chahabi77.pixnet.net/blog/post/386442944\n",
    "    - http://chahabi77.pixnet.net/blog/post/235296791\n",
    "- makeup.txt\n",
    "    - http://bowpisces.pixnet.net/blog/post/64162310\n",
    "    - http://bowpisces.pixnet.net/blog/post/162504740\n",
    "    - http://bowpisces.pixnet.net/blog/post/152118460\n",
    "    - http://bowpisces.pixnet.net/blog/post/142095463\n",
    "    - http://bowpisces.pixnet.net/blog/post/169751742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseRaw(json_map):\n",
    "    url = json_map['url']\n",
    "    content = json_map['html']\n",
    "    return (url,content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 BeautifulSoup 擷取 HTML 內容，並套用 Jieba 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getContent: for input aritcle, get it own word set via jieba.cut()\n",
    "def getContent(x):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(x)\n",
    "    text = soup.getText().replace('\\n','').replace('\\r','').replace(' ','').replace('\\t','')\n",
    "    import jieba\n",
    "    r = list()\n",
    "    for term in jieba.cut(text):\n",
    "        if len(term) > 1 and checkword(term): r.append(term)\n",
    "    return r\n",
    "\n",
    "def checkword(x):\n",
    "    return all(u'\\u4e00' <= c <= u'\\u9fff' for c in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取得 (url,content) List RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "travel_content = sc.textFile(\"./pixnet.txt\").map(json.loads).map(parseRaw)\n",
    "makeup_content = sc.textFile(\"./makeup.txt\").map(json.loads).map(parseRaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 印出文章列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print travel_content.map(lambda x:x[0]).collect()\n",
    "print makeup_content.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 取得 (Link, List(Token))，並計算總Token 數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "travel_token = travel_content.map(lambda x : (x[0], getContent(x[1])))\n",
    "makeup_token = makeup_content.map(lambda x : (x[0], getContent(x[1])))\n",
    "\n",
    "def countTokens(tokenRDD):\n",
    "    return tokenRDD.map(lambda (a ,b): len(b)).reduce(lambda a, b: a + b)\n",
    "\n",
    "totalTokens = countTokens(travel_token) + countTokens(makeup_token)\n",
    "print 'There are %s tokens in full datasets' % totalTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算最多斷詞的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainRDD = travel_token.union(makeup_token)\n",
    "\n",
    "def findBiggestArticle(fullRDD):\n",
    "    return fullRDD.sortBy(lambda x: -len(x[1])).take(1)\n",
    "\n",
    "biggestArticle = findBiggestArticle(trainRDD)\n",
    "print 'The biggest article with Link \"%s\" has the most tokens (%s)' % (biggestArticle[0][0],\n",
    "                                                                   len(biggestArticle[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 分數計算\n",
    "\n",
    "TF-IDF 是一種常用於 Data-Mining 的文章權重計算方法，分別衡量一個斷詞在一篇文章及整個文件集的重要程度。\n",
    "- `IDF`(inverse document frequency): 當一個詞在越少文章出現，其出現對文章的重要性就越大。\n",
    "- `TF`(term frequency): 當一個詞在單篇文章出現的頻率越大，其對文章的重要性也越大。\n",
    "\n",
    "對於每一篇文章，將其內部所有 token 的 TF、IDF值求出並相乘，將會產生這篇文章的 weighted vector。\n",
    "最終，可以透過計算兩篇文章的 cosine similarity ，判斷這兩篇文章的相似度。\n",
    "\n",
    "##### Cosine Similarity\n",
    "- 兩篇文章重複的 token 越多，相似度越高。\n",
    "- 兩篇文章重複的 token 若其原本 TF-IDF 值越高，也會對 cosine similarity 貢獻更多分數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf(tokens):\n",
    "    d = {}\n",
    "    for word in tokens:\n",
    "        if not word in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    for word in d:\n",
    "        d[word] = float(d[word])/len(tokens)\n",
    "    return d\n",
    "\n",
    "travel_token_TF = travel_token.map(lambda record: tf(record[1]))\n",
    "example_dict = travel_token_TF.take(1)[0]\n",
    "example_dict_sorted = sorted(example_dict, key=example_dict.get, reverse=True)\n",
    "\n",
    "print \"Show 10 tokens with the higest frequency.\"\n",
    "for index in range(0,9):\n",
    "    print example_dict_sorted[index], example_dict[example_dict_sorted[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idfs(RDD):\n",
    "    N = RDD.count()\n",
    "    uniqueTokens = RDD.map(lambda x: list(set(x[1])))\n",
    "    tokenSumPairTuple = uniqueTokens.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda a, b : a + b)\n",
    "    return (tokenSumPairTuple.map(lambda x: (x[0], float(N)/x[1])))\n",
    "\n",
    "idfsTrain = idfs(trainRDD)\n",
    "idfsTrainWeights = idfsTrain.collectAsMap()\n",
    "uniqueTokenCount = idfsTrain.count()\n",
    "\n",
    "print 'There are %s unique tokens in the training datasets.' % uniqueTokenCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出20個每篇文章都有出現的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDFTokens = idfsTrain.filter(lambda token: token[1] == 1).take(20)  ##takeOrdered(10, lambda s: -s[1])\n",
    "for token in IDFTokens:\n",
    "    print token[0] + \" \" + str(token[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列出20個只在一篇文章出現的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDFTokens = idfsTrain.filter(lambda token: token[1] == 10).take(20)\n",
    "for token in IDFTokens:\n",
    "    print token[0] + \" \" + str(token[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算出一篇文章的 TF-IDF vector，回傳成 Dictionary 的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf(tokens, idfs):\n",
    "    tfs = tf(tokens)\n",
    "    for tk in tfs:\n",
    "        tfs[tk] = tfs[tk]*idfs[tk]\n",
    "    tfIdfDict = tfs\n",
    "    return tfIdfDict\n",
    "\n",
    "def showTopWord(link):\n",
    "    tokens = trainRDD.filter(lambda x: x[0] == link).collect()[0][1]\n",
    "    tokens_weights = tfidf(tokens, idfsTrainWeights)\n",
    "    tokens_weights_sorted = sorted(tokens_weights, key=tokens_weights.get, reverse=True)\n",
    "    for index in range(0,9):\n",
    "        print tokens_weights_sorted[index], tokens_weights[tokens_weights_sorted[index]]\n",
    "    \n",
    "link = u'http://lohas.pixnet.net/blog/post/31969271'\n",
    "showTopWord(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實作 Dot Product 涵式，透過 cosine similarity 計算文章相似度的分數\n",
    "\n",
    "dotprod\n",
    "- 針對兩篇文章的tfidf() dictionary，針對所有共同 key 的值做相乘，並將結果加總\n",
    "\n",
    "norm\n",
    "- 計算 cosine similarity 的 square root\n",
    "\n",
    "cossim\n",
    "- 計算兩篇文章的 cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    dotsum = 0\n",
    "    for tk in a:\n",
    "        if tk in b:\n",
    "            dotsum += a[tk]*b[tk]\n",
    "    return dotsum\n",
    "\n",
    "def norm(a):\n",
    "    return math.sqrt(dotprod(a,a))\n",
    "\n",
    "def cossim(a, b):\n",
    "    return dotprod(a,b)/(norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    w1 = tfidf(string1, idfsDictionary)\n",
    "    w2 = tfidf(string2, idfsDictionary)\n",
    "    return cossim(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 10 篇文章的 Cartesian Coordinate ，並計算每兩篇文章的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossPair = (trainRDD\n",
    "              .cartesian(trainRDD)\n",
    "              .cache())\n",
    "crossPair.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarities = (crossPair \n",
    "                .map(lambda record: \n",
    "                     (record[0][0], record[1][0], cosineSimilarity(record[0][1], record[1][1], idfsTrainWeights)))\n",
    "                .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSimilar(link):\n",
    "    return (similarities\n",
    "            .filter(lambda record: (record[0] == link))\n",
    "            .map(lambda record: (record[1], record[2]))\n",
    "            .sortBy(lambda x: -x[1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸入一篇文章，取得三篇最相似文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarArticle = getSimilar(u'http://bowpisces.pixnet.net/blog/post/152118460')\n",
    "for index in range(1, 4):\n",
    "    print similarArticle[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###探索文章裡的關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showTopWord(u'http://bowpisces.pixnet.net/blog/post/162504740')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarArticle = getSimilar(u'http://lohas.pixnet.net/blog/post/31969271')\n",
    "for index in range(1, 4):\n",
    "    print similarArticle[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showTopWord(u'http://lohas.pixnet.net/blog/post/32286983')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
